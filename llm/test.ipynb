{"cells":[{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import warnings\n","\n","from dotenv import load_dotenv\n","from langchain_community.chat_models import ChatOpenAI\n","\n","\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n","load_dotenv()\n","\n","\n","llm = ChatOpenAI(api_key=\"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\")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["AIMessage(content='Langsmith can help with testing in several ways:\\n\\n1. Test Automation: Langsmith can generate test cases automatically based on the defined specifications and requirements. It can create a variety of test scenarios and data combinations to cover different aspects of the system under test. This saves time and effort in manual test case creation and execution.\\n\\n2. Test Coverage: Langsmith can analyze the codebase and identify areas that have not been adequately covered by tests. It can suggest additional test cases or test scenarios to improve the overall test coverage.\\n\\n3. Test Oracles: Langsmith can act as an intelligent test oracle by comparing the expected behavior of the system with the actual behavior during testing. It can identify discrepancies and highlight potential bugs or issues in the software.\\n\\n4. Test Case Prioritization: Langsmith can prioritize test cases based on the impact and likelihood of failure. This helps in optimizing the testing process by focusing on high-risk areas first, ensuring critical bugs are detected early.\\n\\n5. Test Suite Optimization: Langsmith can optimize the test suite by identifying redundant or overlapping test cases. It can suggest the removal of duplicate tests, reducing the execution time while maintaining the same level of coverage.\\n\\n6. Regression Testing: Langsmith can assist in regression testing by automatically retesting the modified code and comparing it with the expected behavior. It can identify if any new bugs or issues have been introduced due to code changes.\\n\\nOverall, Langsmith can enhance the efficiency and effectiveness of testing by automating repetitive tasks, improving test coverage, and providing intelligent insights into the testing process.')"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["llm.invoke(\"how can langsmith help with testing?\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["from langchain.prompts import ChatPromptTemplate\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"You are world class technical documentation writer.\"),\n","    (\"user\", \"{input}\")\n","])"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["chain = prompt | llm "]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["AIMessage(content=\"Langsmith can be a valuable tool for testing in various ways. Here are a few ways in which Langsmith can help with testing:\\n\\n1. Test Case Generation: Langsmith can generate test cases automatically based on the specifications or requirements of the system under test. By understanding the desired behavior of the system, Langsmith can create a comprehensive set of test cases, covering different scenarios and edge cases.\\n\\n2. Test Data Generation: Testing often requires a wide range of test data to cover different input combinations. Langsmith can assist in generating diverse and relevant test data, ensuring that all possible input variations are considered during testing. This can help in discovering potential issues or bugs that might occur with different data inputs.\\n\\n3. Test Execution Automation: Langsmith can automate the execution of test cases, making the testing process more efficient. It can interact with the system under test, perform actions, and verify the expected outcomes. This automation saves time and effort, allowing testers to focus on more complex and critical aspects of testing.\\n\\n4. Performance Testing: Langsmith can simulate a large number of users or load on a system to test its performance under various conditions. By generating realistic user interactions and load patterns, Langsmith can help identify performance bottlenecks, scalability issues, and resource utilization problems.\\n\\n5. Regression Testing: Whenever changes are made to a system, regression testing is necessary to ensure that existing functionality has not been affected. Langsmith can help automate regression testing by re-executing previously defined test cases, thereby saving time and effort compared to manual regression testing.\\n\\n6. Test Reporting: Langsmith can generate detailed test reports, providing insights into test coverage, test results, and any issues encountered during testing. These reports can be used to track the progress of testing, communicate testing outcomes to stakeholders, and facilitate decision-making.\\n\\nOverall, Langsmith's capabilities in test case generation, test data generation, test execution automation, performance testing, regression testing, and test reporting make it a valuable tool in the testing process, enhancing efficiency and improving the quality of software products.\")"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["chain.invoke({\"input\": \"how can langsmith help with testing?\"})"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["from langchain_core.output_parsers import StrOutputParser\n","\n","output_parser = StrOutputParser()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["chain = prompt | llm | output_parser"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["'Langsmith can be a valuable tool for testing purposes in several ways:\\n\\n1. Test Case Generation: Langsmith can automatically generate test cases based on the specifications of a system or software. It uses language models to analyze the input/output behavior and generate a set of test cases that cover different scenarios and edge cases. This helps in ensuring comprehensive test coverage.\\n\\n2. Test Data Generation: Langsmith can generate realistic and diverse test data to be used during testing. It can create a variety of input values for different data types, including strings, numbers, dates, and more. This helps in validating the behavior of the system under different input conditions.\\n\\n3. Test Automation: Langsmith can be integrated with testing frameworks and tools to automate the execution of test cases. It provides APIs and libraries that allow testers to write test scripts in their preferred programming languages. This enables the creation of automated test suites that can be run repeatedly to ensure the stability and reliability of the system.\\n\\n4. Test Documentation: Langsmith can help in creating test documentation by automatically generating test cases, test data, and test scripts. It can produce clear and concise test reports that include the details of the executed tests, their results, and any identified issues. This helps in maintaining a comprehensive record of the testing process.\\n\\n5. Performance Testing: Langsmith can simulate high volumes of concurrent users or requests to test the performance and scalability of a system. By generating realistic user behavior and load patterns, it helps in identifying any bottlenecks or performance issues in the system.\\n\\nOverall, Langsmith simplifies and streamlines the testing process by providing automated solutions for test case generation, test data generation, test automation, and performance testing. It enhances the efficiency and effectiveness of the testing process, leading to improved software quality.'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["chain.invoke({\"input\": \"how can langsmith help with testing?\"})"]},{"cell_type":"code","execution_count":12,"metadata":{"vscode":{"languageId":"shellscript"}},"outputs":[{"data":{"text/plain":["\"Langsmith can greatly assist with testing by providing a powerful language model that can generate test cases, simulate user interactions, and even assist with bug identification and debugging. Here are several ways in which Langsmith can help with testing:\\n\\n1. Test Case Generation: Langsmith can generate a wide range of test cases based on given specifications or requirements. By utilizing the model's language understanding capabilities, it can create test inputs, boundary values, and edge cases to ensure comprehensive test coverage.\\n\\n2. Test Data Generation: With Langsmith, you can generate realistic and diverse test data. The model can generate random data, populate databases, create mock API responses, and much more. This helps in testing the system with different types of data to uncover potential issues.\\n\\n3. User Interaction Simulation: Langsmith can simulate user interactions with the system, allowing you to automate the testing process. By providing instructions to the model, you can script user actions, validate system responses, and detect any unexpected behavior or errors.\\n\\n4. Bug Identification and Debugging: Langsmith can analyze error logs and provide suggestions for potential fixes or debugging strategies. By feeding the model with relevant information about the issue, it can assist in pinpointing the root cause and propose potential solutions.\\n\\n5. Documentation and Reporting: Langsmith can help in generating test reports and documentation. It can generate test plans, test scripts, and test summaries, making it easier to document and communicate the testing process and results.\\n\\n6. Training and Education: Langsmith can act as a virtual instructor, providing explanations and examples on testing concepts and best practices. It can answer questions related to testing techniques, methodologies, and frameworks, helping testers improve their skills.\\n\\nRemember, while Langsmith can be a valuable tool in testing, it is important to combine its capabilities with human expertise and validation to ensure the accuracy and reliability of the testing process.\""]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["\"Langsmith can greatly assist with testing by providing a powerful language model that can generate test cases, simulate user interactions, and even assist with bug identification and debugging. Here are several ways in which Langsmith can help with testing:\\n\\n1. Test Case Generation: Langsmith can generate a wide range of test cases based on given specifications or requirements. By utilizing the model's language understanding capabilities, it can create test inputs, boundary values, and edge cases to ensure comprehensive test coverage.\\n\\n2. Test Data Generation: With Langsmith, you can generate realistic and diverse test data. The model can generate random data, populate databases, create mock API responses, and much more. This helps in testing the system with different types of data to uncover potential issues.\\n\\n3. User Interaction Simulation: Langsmith can simulate user interactions with the system, allowing you to automate the testing process. By providing instructions to the model, you can script user actions, validate system responses, and detect any unexpected behavior or errors.\\n\\n4. Bug Identification and Debugging: Langsmith can analyze error logs and provide suggestions for potential fixes or debugging strategies. By feeding the model with relevant information about the issue, it can assist in pinpointing the root cause and propose potential solutions.\\n\\n5. Documentation and Reporting: Langsmith can help in generating test reports and documentation. It can generate test plans, test scripts, and test summaries, making it easier to document and communicate the testing process and results.\\n\\n6. Training and Education: Langsmith can act as a virtual instructor, providing explanations and examples on testing concepts and best practices. It can answer questions related to testing techniques, methodologies, and frameworks, helping testers improve their skills.\\n\\nRemember, while Langsmith can be a valuable tool in testing, it is important to combine its capabilities with human expertise and validation to ensure the accuracy and reliability of the testing process.\""]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["from langchain_community.document_loaders import WebBaseLoader\n","loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n","\n","docs = loader.load()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["from langchain_community.embeddings import OpenAIEmbeddings\n","\n","embeddings = OpenAIEmbeddings(api_key=\"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\")"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["from langchain_community.vectorstores import DocArrayInMemorySearch\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","\n","text_splitter = RecursiveCharacterTextSplitter()\n","documents = text_splitter.split_documents(docs)\n","vector = DocArrayInMemorySearch.from_documents(documents, embeddings)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["from langchain.chains.combine_documents import create_stuff_documents_chain\n","\n","prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n","\n","<context>\n","{context}\n","</context>\n","\n","Question: {input}\"\"\")\n","\n","document_chain = create_stuff_documents_chain(llm, prompt)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["'Langsmith can help with testing by allowing users to visualize test results.'"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["from langchain_core.documents import Document\n","\n","document_chain.invoke({\n","    \"input\": \"how can langsmith help with testing?\",\n","    \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]\n","})"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["from langchain.chains import create_retrieval_chain\n","\n","retriever = vector.as_retriever()\n","retrieval_chain = create_retrieval_chain(retriever, document_chain)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["LangSmith can help with testing by providing tools and features that simplify the process of testing changes to prompts or chains. It allows users to construct datasets and run chains over the data points, logging the results and visualizing the outputs. Users can also assign feedback to runs and mark them as correct or incorrect, providing aggregate statistics for each test project. Additionally, LangSmith offers evaluators that can be specified during a test run to evaluate the results. While these evaluators may not be perfect, they can guide users to examples that should be examined. LangSmith also facilitates human evaluation by providing annotation queues, where runs can be selected based on criteria and queued up for human review. Reviewers can view the input, output, and existing tags before adding their own feedback. These annotations are then assigned as \"feedback\" to the source runs, allowing for easy filtering and analysis.\n"]}],"source":["response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n","print(response[\"answer\"])\n","\n","# LangSmith offers several features that can help with testing:..."]},{"cell_type":"markdown","metadata":{},"source":["Retriaval"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["from langchain.chains import create_history_aware_retriever\n","from langchain_core.prompts import MessagesPlaceholder\n","\n","# First we need a prompt that we can pass into an LLM to generate this search query\n","\n","prompt = ChatPromptTemplate.from_messages([\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"user\", \"{input}\"),\n","    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n","])\n","retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["[Document(page_content='LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n"," Document(page_content=\"Skip to main contentğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by defaultâ€‹At LangChain, all of us have LangSmithâ€™s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we donâ€™t even look at the traces, but the 10% of the time that we doâ€¦ itâ€™s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebuggingâ€‹Debugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?â€‹LLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string â†’ string (or chat messages â†’ chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?â€‹So you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?â€‹In complicated chains and agents, it can often be hard to understand what is going on under the hood. What\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n"," Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoringâ€‹After all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.Weâ€™ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing â€” mirroring the debug mode approach.Weâ€™ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasetsâ€‹LangSmith makes it easy to curate datasets. However, these arenâ€™t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.â†©PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n"," Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat weâ€™ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If weâ€™re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluationâ€‹Automatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})]"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["from langchain_core.messages import HumanMessage, AIMessage\n","\n","chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n","retriever_chain.invoke({\n","    \"chat_history\": chat_history,\n","    \"input\": \"Tell me how\"\n","})"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"user\", \"{input}\"),\n","])\n","document_chain = create_stuff_documents_chain(llm, prompt)\n","\n","retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/plain":["{'chat_history': [HumanMessage(content='Can LangSmith help test my LLM applications?'),\n","  AIMessage(content='Yes!')],\n"," 'input': 'Tell me how',\n"," 'context': [Document(page_content='LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n","  Document(page_content=\"Skip to main contentğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by defaultâ€‹At LangChain, all of us have LangSmithâ€™s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we donâ€™t even look at the traces, but the 10% of the time that we doâ€¦ itâ€™s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebuggingâ€‹Debugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?â€‹LLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string â†’ string (or chat messages â†’ chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?â€‹So you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?â€‹In complicated chains and agents, it can often be hard to understand what is going on under the hood. What\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n","  Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoringâ€‹After all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.Weâ€™ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing â€” mirroring the debug mode approach.Weâ€™ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasetsâ€‹LangSmith makes it easy to curate datasets. However, these arenâ€™t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.â†©PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n","  Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat weâ€™ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If weâ€™re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluationâ€‹Automatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})],\n"," 'answer': 'LangSmith can help test your LLM applications by providing various features and functionalities. Here are a few ways LangSmith can assist you in testing:\\n\\n1. Tracing: LangSmith records and logs all calls to LLMs, chains, agents, tools, and retrievers. This tracing feature allows you to monitor and debug your application by providing insights into unexpected end results, agent looping, chain performance, and token usage.\\n\\n2. Debugging: Debugging LLMs, chains, and agents can be challenging, but LangSmith simplifies the process. It provides a straightforward visualization of the exact inputs and outputs of LLM calls, helping you understand the data flow and identify any formatting issues or unexpected transformations.\\n\\n3. Prompt Editing: If you want to test how editing the prompt affects the output, LangSmith offers a playground feature. You can modify the prompt and re-run it multiple times to observe the resulting changes in the output. This feature currently supports OpenAI and Anthropic models for LLM and Chat Model calls.\\n\\n4. Sequence of Events: In complex chains and agents, understanding the exact sequence of events can be difficult. LangSmith helps by providing visibility into the sequence of events, allowing you to track the flow of data and identify any issues or bottlenecks.\\n\\n5. Dataset Testing: LangSmith simplifies dataset uploading and allows you to test changes to prompts or chains using a dataset. You can run the chain over the data points, visualize the outputs, and assign feedback to the runs. LangSmith also provides evaluators to analyze the results and guide your attention to specific examples.\\n\\n6. Human Evaluation: While automatic evaluation metrics are useful, human review is crucial for assessing subjective qualities and ensuring the utmost quality and reliability of your application. LangSmith offers annotation queues that allow you to manually review and annotate runs, providing valuable feedback. These annotations are assigned as feedback to the source runs for further analysis.\\n\\nOverall, LangSmith provides a comprehensive testing framework to evaluate and improve the performance of your LLM applications.'}"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n","retrieval_chain.invoke({\n","    \"chat_history\": chat_history,\n","    \"input\": \"Tell me how\"\n","})"]},{"cell_type":"markdown","metadata":{},"source":["# Agent"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["from langchain.tools.retriever import create_retriever_tool\n","\n","retriever_tool = create_retriever_tool(\n","    retriever,\n","    \"langsmith_search\",\n","    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",")"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["from langchain_community.tools.tavily_search import TavilySearchResults\n","load_dotenv() \n","search = TavilySearchResults()"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["tools = [retriever_tool, search]"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["from langchain_community.chat_models import ChatOpenAI\n","from langchain import hub\n","from langchain.agents import create_openai_functions_agent\n","from langchain.agents import AgentExecutor\n","\n","# Get the prompt to use - you can modify this!\n","prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n","llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n","agent = create_openai_functions_agent(llm, tools, prompt)\n","agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["Authentication failed for https://api.smith.langchain.com/runs/e88452c2-666f-4db4-adc3-31bbe6227552. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/e88452c2-666f-4db4-adc3-31bbe6227552', '{\"detail\":\"Invalid auth\"}')\n","Authentication failed for https://api.smith.langchain.com/runs. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs', '{\"detail\":\"Invalid auth\"}')\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[32;1m\u001b[1;3mLangSmith can help with testing in several ways:\n","\n","1. Test Case Generation: LangSmith can generate test cases automatically based on the specifications of the software being tested. It uses advanced techniques such as symbolic execution and constraint solving to generate test inputs that cover different execution paths and edge cases.\n","\n","2. Test Oracles: LangSmith can help in defining test oracles, which are the expected outputs or behaviors of the software under test. It can analyze the specifications and requirements of the software to automatically generate assertions or checks that can be used as test oracles.\n","\n","3. Test Coverage Analysis: LangSmith can analyze the coverage of test cases to determine which parts of the software have been exercised during testing. It can provide insights into the effectiveness of the test suite and identify areas that need additional testing.\n","\n","4. Test Prioritization: LangSmith can prioritize test cases based on their likelihood of finding bugs or their impact on the system. It can use techniques such as code coverage analysis, fault localization, and mutation analysis to prioritize the execution of test cases.\n","\n","5. Test Result Analysis: LangSmith can analyze the results of test executions to identify patterns or trends that can help in debugging and fixing issues. It can provide insights into the root causes of failures and suggest possible fixes or workarounds.\n","\n","Overall, LangSmith can assist in automating various aspects of the testing process, making it more efficient and effective.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"data":{"text/plain":["{'input': 'how can langsmith help with testing?',\n"," 'output': 'LangSmith can help with testing in several ways:\\n\\n1. Test Case Generation: LangSmith can generate test cases automatically based on the specifications of the software being tested. It uses advanced techniques such as symbolic execution and constraint solving to generate test inputs that cover different execution paths and edge cases.\\n\\n2. Test Oracles: LangSmith can help in defining test oracles, which are the expected outputs or behaviors of the software under test. It can analyze the specifications and requirements of the software to automatically generate assertions or checks that can be used as test oracles.\\n\\n3. Test Coverage Analysis: LangSmith can analyze the coverage of test cases to determine which parts of the software have been exercised during testing. It can provide insights into the effectiveness of the test suite and identify areas that need additional testing.\\n\\n4. Test Prioritization: LangSmith can prioritize test cases based on their likelihood of finding bugs or their impact on the system. It can use techniques such as code coverage analysis, fault localization, and mutation analysis to prioritize the execution of test cases.\\n\\n5. Test Result Analysis: LangSmith can analyze the results of test executions to identify patterns or trends that can help in debugging and fixing issues. It can provide insights into the root causes of failures and suggest possible fixes or workarounds.\\n\\nOverall, LangSmith can assist in automating various aspects of the testing process, making it more efficient and effective.'}"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","Invoking: `tavily_search_results_json` with `{'query': 'current state of Artificial Synapse'}`\n","\n","\n","\u001b[0m\u001b[33;1m\u001b[1;3m[{'url': 'https://www.nature.com/articles/s41378-023-00487-2', 'content': 'on the current development of artificial synapse properties excited by electrical, optical, mechanical, and hybrid  Recent progress in three-terminal artificial synapses based on 2D materials: from mechanisms to applications  Scott T. Keene, Claudia Lubrano, â€¦ Francesca Santoro An artificial synapse based on molecular junctions  biological neural behaviors129. Finally, the challenges and future perspectives of artificial synapses are outlined.At present, research on three- and multiple-terminal synaptic transistors is still preliminary, and neuromorphic transistors have mainly focused on four types: synapses based on floating...'}, {'url': 'https://hai.stanford.edu/news/what-expect-ai-2024', 'content': 'What to Expect in AI in 2024  not only to foster a vibrant AI ecosystem but also to harness and govern the technology. I hope in 2024, we’ll see  and try to put guidelines down on a page in 2024.  to actually do things. 2023 was the year of being able to chat with an AI.AI policy will be worth watching in 2024. We saw the most progress in 2023 to date. In July, Congress introduced the bipartisan, bicameral CREATE AI Act to give students and researchers access to AI resources, data, and tools. It garnered widespread support because it promises to broaden access to AI development.'}, {'url': 'https://thenextweb.com/news/ai-predictions-2024', 'content': 'Experts share their predictions for artificial intelligence in the year to come  After a year of breathless hype, AI will face reality in 2024  in organisations’ adoption of these technologies,” McKinsey concluded in its report on the state of AI in 2023.  for the sector. In 2024, other emerging techniques could take advantage of the GenAI boom.December 28, 2023 - 8:00 am. Artificial intelligence went mainstream in 2023. The watershed moment arrived at the end of the previous year, with the November 30 release of ChatGPT. Just two months ...'}, {'url': 'https://theconversation.com/ai-is-here-and-everywhere-3-ai-researchers-look-to-the-challenges-ahead-in-2024-218218', 'content': 'this problem, so I expect some headway in 2024.  Edition AI is here – and everywhere: 3 AI researchers look to the challenges ahead in\\xa02024 Authors  In the year since the unveiling of ChatGPT, the development of generative AI models is continuing at a dizzying pace.  the singularity, the moment artificial intelligence matches and begins to exceed human intelligence – not quite here2023 was an inflection point in the evolution of artificial intelligence and its role in society. ... AI scholars to look ahead to 2024 and describe the issues AI developers, regulators and ...'}, {'url': 'https://www.latimes.com/business/technology/story/2024-01-02/ai-predictions-2024-competency-tests-election-ads-bankruptcies', 'content': 'Could 2024 be the year we start holding AI to the same standard?  California Worried about AI? How California lawmakers plan to tackle the technology’s risks in 2024 Dec. 28, 2023  breakthroughs in AI research and development.  Jan. 2, 2024 Technology and the Internet Musk’s X fails to block California content moderation law Dec. 29, 2023What AI will bring in 2024: 4 predictions. One thing both skeptics and enthusiasts alike can agree on: Artificial intelligence will have a banner year in 2024. (Jim Cooke / Los Angeles Times) By ...'}]\u001b[0m\u001b[32;1m\u001b[1;3m\n","Invoking: `tavily_search_results_json` with `{'query': 'main patents in Artificial Synapse field'}`\n","\n","\n","\u001b[0m\u001b[33;1m\u001b[1;3m[{'url': 'https://www.nature.com/articles/s41378-023-00487-2', 'content': 'on the current development of artificial synapse properties excited by electrical, optical, mechanical, and hybrid  Recent progress in three-terminal artificial synapses based on 2D materials: from mechanisms to applications  artificial synapse applications in various sensory systems, including bioplastic bionics, logical transformation,  Scott T. Keene, Claudia Lubrano, â€¦ Francesca Santoro An artificial synapse based on molecular junctionsAt present, research on three- and multiple-terminal synaptic transistors is still preliminary, and neuromorphic transistors have mainly focused on four types: synapses based on floating...'}, {'url': 'https://www.nature.com/articles/s41528-020-0066-0', 'content': 'in the design and fabrication of artificial synapses.  new avenue for the construction of biocompatible electronic devices towards artificial intelligence systems.  Similar content being viewed by others Biological receptor-inspired flexible artificial synapse based on ionic dynamics  and JavaScript. Advertisement Bio-inspired flexible artificial synapses for pain perception and nerve injuries1 Altmetric Metrics Abstract Imitation of the perception system of living creatures is of great importance for the construction of artificial nerves and intelligent human-machine interfaces....'}, {'url': 'https://www.nature.com/articles/s41378-020-00189-z', 'content': 'High sensitivity and wide response range artificial synapse based on polyimide with embedded graphene quantum dots  memristor-type artificial synaptic device shows great potential in neuromorphic networks.  on the historical applied voltages are observed, offering a promising pathway toward biorealistic synaptic emulation.  and JavaScript. Advertisement Biological receptor-inspired flexible artificial synapse based on ionic dynamicsIntroduction Inspired by the massive parallelism, robust computation, fault tolerance, and energy efficiency of the human brain, neuromorphic computing has attracted a tremendous upsurge of...'}, {'url': 'https://link.springer.com/article/10.1007/s40820-021-00618-2', 'content': 'Memristive Artificial Synapses for Neuromorphic Computing You have full access to this open access article 13k Accesses  synaptic devices that could be significant for building efficient neuromorphic systems are prospected.  The working mechanisms, progress, and application scenarios of synaptic devices based on electrical and optical signals  building efficient neuromorphic systems are prospected.Brain-like computing for learning and information processing utilizes the plasticity of neural synapses in the human brain neural network [].Each synapse is composed of the presynaptic terminal, postsynaptic terminal, and the narrow gap between the two terminals, that is, the synaptic cleft (Fig. 1a) [].The synapse is where neurons are functionally connected and is the key component for ...'}, {'url': 'https://www.nature.com/articles/s41467-023-35817-5', 'content': 'and JavaScript. Advertisement An artificial synapse based on molecular junctions Nature Communications  artificial synapse within one molecule length scale. This molecular synapse is subsequently used in reservoir computing  3D spatiotemporally scalable in vivo neural probes based on fluorinated elastomers  Hongwei Cai, Zheng Ao, â€¦ Feng Guo MoirÃ© synaptic transistor with room-temperature neuromorphic functionalityShrinking the size of the electronic synapse to molecular length-scale, for example, an artificial synapse directly fabricated by using individual or monolayer molecules, is important for ...'}]\u001b[0m\u001b[32;1m\u001b[1;3mThe current state of Artificial Synapse is still in its preliminary stages. Research is mainly focused on three- and multiple-terminal synaptic transistors. There are four main types of synapses being studied: those based on electrical, optical, mechanical, and hybrid properties. The development of artificial synapses is crucial for the construction of biocompatible electronic devices towards artificial intelligence systems. Some of the applications of artificial synapses include various sensory systems, bioplastic bionics, and logical transformation [^1^].\n","\n","As for patents in the field of Artificial Synapse, it's a bit more complex. There are several significant works in the field. For instance, there is research on artificial synapses based on polyimide with embedded graphene quantum dots. This memristor-type artificial synaptic device shows great potential in neuromorphic networks [^3^]. Another study discusses the construction of an artificial synapse within one molecule length scale, which is used in reservoir computing [^5^]. There's also work on synaptic devices based on electrical and optical signals that could be significant for building efficient neuromorphic systems [^4^].\n","\n","Please note that while these are significant works in the field, it's not clear if they are patented or not. For specific patent information, you may need to consult a patent database or a legal professional.\n","\n","[^1^]: [Nature](https://www.nature.com/articles/s41378-023-00487-2)\n","[^3^]: [Nature](https://www.nature.com/articles/s41378-020-00189-z)\n","[^4^]: [Springer](https://link.springer.com/article/10.1007/s40820-021-00618-2)\n","[^5^]: [Nature](https://www.nature.com/articles/s41467-023-35817-5)\n","\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"data":{"text/plain":["{'input': 'I want to go deeper into neuroscience. What is the current state of Artifical Synapse? What are main patent in that field?',\n"," 'output': \"The current state of Artificial Synapse is still in its preliminary stages. Research is mainly focused on three- and multiple-terminal synaptic transistors. There are four main types of synapses being studied: those based on electrical, optical, mechanical, and hybrid properties. The development of artificial synapses is crucial for the construction of biocompatible electronic devices towards artificial intelligence systems. Some of the applications of artificial synapses include various sensory systems, bioplastic bionics, and logical transformation [^1^].\\n\\nAs for patents in the field of Artificial Synapse, it's a bit more complex. There are several significant works in the field. For instance, there is research on artificial synapses based on polyimide with embedded graphene quantum dots. This memristor-type artificial synaptic device shows great potential in neuromorphic networks [^3^]. Another study discusses the construction of an artificial synapse within one molecule length scale, which is used in reservoir computing [^5^]. There's also work on synaptic devices based on electrical and optical signals that could be significant for building efficient neuromorphic systems [^4^].\\n\\nPlease note that while these are significant works in the field, it's not clear if they are patented or not. For specific patent information, you may need to consult a patent database or a legal professional.\\n\\n[^1^]: [Nature](https://www.nature.com/articles/s41378-023-00487-2)\\n[^3^]: [Nature](https://www.nature.com/articles/s41378-020-00189-z)\\n[^4^]: [Springer](https://link.springer.com/article/10.1007/s40820-021-00618-2)\\n[^5^]: [Nature](https://www.nature.com/articles/s41467-023-35817-5)\\n\"}"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["agent_executor.invoke({\"input\": \"I want to go deeper into neuroscience. What is the current state of Artifical Synapse? What are main patent in that field?\"})"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12"}},"nbformat":4,"nbformat_minor":2}
